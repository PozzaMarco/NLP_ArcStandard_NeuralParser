{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c041f954-42d3-44e6-b2c2-281d579e61e6",
      "metadata": {
        "id": "c041f954-42d3-44e6-b2c2-281d579e61e6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install conllu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ace40b65-a58f-47b6-9905-f41ad6a8d12f",
      "metadata": {
        "id": "ace40b65-a58f-47b6-9905-f41ad6a8d12f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import itertools\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d704ceb0",
      "metadata": {
        "id": "d704ceb0"
      },
      "source": [
        "# Bert model and other hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cad5b81d",
      "metadata": {
        "id": "cad5b81d"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "BERT_MODEL = 'dbmdz/bert-base-italian-xxl-cased'\n",
        "DROPOUT_RATE = 0.5\n",
        "MLP_SIZE = 300\n",
        "INPUT_SIZE = 3*768\n",
        "LEARNING_RATE = 0.002\n",
        "EPOCHS = 5\n",
        "SAVE_PATH = \"saved_checkpoint.pt\"\n",
        "TOKENIZER_MAX_LEN = 150\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49ac7b5d-012a-40dc-9d04-6ab4cfcca9a5",
      "metadata": {
        "id": "49ac7b5d-012a-40dc-9d04-6ab4cfcca9a5"
      },
      "source": [
        "# Arc-standard\n",
        "\n",
        "Recall that a **configuration** of the arc-standard parser is a triple of the form $( \\sigma, \\beta, A)$\n",
        "where:\n",
        "\n",
        "* $\\sigma$ is the stack;\n",
        "* $\\beta$ is the input buffer;\n",
        "* $A$ is a set of arcs constructed so far.\n",
        "\n",
        "We write $\\sigma_i$, $i \\geq 1$, for the $i$-th token in the stack; we also write $\\beta_i$, $i \\geq 1$, for the $i$-th token in the buffer. \n",
        "\n",
        "The parser can perform three types of **actions** (transitions):\n",
        "\n",
        "* **shift**, which removes $\\beta_1$ from the buffer and pushes it into the stack;\n",
        "* **left-arc**, which creates the arc $(\\sigma_1 \\rightarrow \\sigma_2)$, and removes $\\sigma_2$ from the stack;\n",
        "* **right-arc**, which creates the arc $(\\sigma_2 \\rightarrow \\sigma_1)$, and removes $\\sigma_1$ from the stack.\n",
        "\n",
        "Let $w = w_0 w_1 \\cdots w_{n}$ be the input sentence, with $w_0$ the special symbol `<ROOT>`.\n",
        "Stack and buffer are implemented as lists of integers, where `j` represents word $w_j$.  Top-most stack token is at the right-end of the list; first buffer token is at the left-end of the list. \n",
        "Set $A$ is implemented as an array `arcs` of size $n+1$ such that if arc $(w_i \\rightarrow w_j)$ is in $A$ then `arcs[j]=i`, and if $w_j$ is still missing its head node in the tree under construction, then `arcs[j]=-1`. We always have `arcs[0]=-1`.  We use this representation also for complete dependency trees.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f68788f-e071-424f-9af7-2a35a4c0c9bf",
      "metadata": {
        "id": "1f68788f-e071-424f-9af7-2a35a4c0c9bf"
      },
      "outputs": [],
      "source": [
        "import orig #file orig.py in the same foldr contains original class and function definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd4e22b9-5e49-4b3e-8ab6-de62b923bae7",
      "metadata": {
        "id": "bd4e22b9-5e49-4b3e-8ab6-de62b923bae7"
      },
      "outputs": [],
      "source": [
        "class ArcStandard (orig.ArcStandard):\n",
        "    def __init__(self, sentence, label_set):\n",
        "        self.sentence = sentence\n",
        "        self.buffer = [i for i in range(len(self.sentence))]\n",
        "        self.stack = []\n",
        "        self.arcs = [(-1, -1) for _ in range(len(self.sentence))]# head - relation converted to int\n",
        "\n",
        "        self.label_set = {y:x for x,y in label_set.items()} #for future reference (printing)\n",
        "        # three shift moves to initialize the stack\n",
        "        self.shift()\n",
        "        self.shift()\n",
        "        if len(self.sentence) > 2:\n",
        "            self.shift()\n",
        "\n",
        "    def left_arc(self, deprel):\n",
        "        o1 = self.stack.pop()\n",
        "        o2 = self.stack.pop()\n",
        "        self.arcs[o2] = (o1, deprel) #added deprel\n",
        "        self.stack.append(o1)\n",
        "        if len(self.stack) < 2 and len(self.buffer) > 0:\n",
        "            self.shift()\n",
        "\n",
        "    def right_arc(self, deprel):\n",
        "        o1 = self.stack.pop()\n",
        "        o2 = self.stack.pop()\n",
        "        self.arcs[o1] = (o2, deprel) #added deprel\n",
        "        self.stack.append(o2)\n",
        "        if len(self.stack) < 2 and len(self.buffer) > 0:\n",
        "            self.shift()\n",
        "\n",
        "    def print_configuration(self):\n",
        "        s = [self.sentence[i] for i in self.stack]\n",
        "        b = [self.sentence[i] for i in self.buffer]\n",
        "        print(\"STACK: \", s, \"BUFFER: \", b) #added indication of stack and buffer\n",
        "        print([(x[0], self.label_set[x[1]]) for x in self.arcs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d836502-bba0-4f49-9c39-39a916ed270c",
      "metadata": {
        "id": "1d836502-bba0-4f49-9c39-39a916ed270c"
      },
      "outputs": [],
      "source": [
        "class Oracle (orig.Oracle):\n",
        "    def __init__(self, parser, gold_tree, gold_labels):\n",
        "        self.parser = parser\n",
        "        self.gold = gold_tree\n",
        "        self.labels = gold_labels #must be integers; see below\n",
        "\n",
        "    def get_gold_label(self):\n",
        "        if(self.is_left_arc_gold()):\n",
        "            o2 = self.parser.stack[len(self.parser.stack)-2]\n",
        "            return self.labels[o2]\n",
        "        elif(self.is_right_arc_gold()):\n",
        "            o1 = self.parser.stack[len(self.parser.stack)-1]\n",
        "            return self.labels[o1]\n",
        "        else:\n",
        "            raise Exception(\"Action SHIFT does not produce a labeled dependency\")\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "747b671a-fd4b-4cbd-a602-a9a25da5e4b2",
      "metadata": {
        "id": "747b671a-fd4b-4cbd-a602-a9a25da5e4b2"
      },
      "source": [
        "## Functions to simplify the processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbd7e0ac-d41b-4bd2-ba9a-4ec7d98f1374",
      "metadata": {
        "id": "fbd7e0ac-d41b-4bd2-ba9a-4ec7d98f1374"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "dataset should be a Dataset object from HuggingFace\n",
        "Return a dictionary where arc labels are associate to integer values.\n",
        "Label '_' is removed (used for composite tokens)\n",
        "Labels are sorted in alphabetical order because MSE in the neural network will penalize less classes that are \"close\".\n",
        "<ROOT> label is given value -1 (label of the <ROOT> node).\n",
        "\"\"\"\n",
        "def create_deprel_dict(dataset):\n",
        "    labels = set()\n",
        "    for x in dataset['deprel']:\n",
        "        for elem in x:\n",
        "            if(elem != '_'):\n",
        "                labels.add(elem)\n",
        "    labels = sorted(list(labels))\n",
        "    ret = {labels[i]:i for i in range(len(labels))}\n",
        "    ret.update({\"None\":-1})\n",
        "    return ret\n",
        "\n",
        "\"\"\"\n",
        "Return (sanitized_tokens, gold tree, gold labels) for a sentence in the dataset, assuming gold tree is in a column labeled 'head' and labels in a column labeled 'deprel'.\n",
        "Both token indices and labels are converted to integer (in the latter case, according to deprel_dict).\n",
        "Sanitized tokens is a tokenlist with removal of composite tokens.\n",
        "\"\"\"\n",
        "def create_gold(sentence, deprel_dict):\n",
        "    sanitized = ['<ROOT>']\n",
        "    gold_tree = [-1]\n",
        "    gold_labels = [-1]\n",
        "    for i in range(len(sentence['tokens'])):\n",
        "        if(sentence['head'][i] != 'None') and sentence['deprel'][i] in deprel_dict: #second condition is to avoid unseen or wrong labels\n",
        "            sanitized.append(sentence['tokens'][i])\n",
        "            gold_tree.append(int(sentence['head'][i]))\n",
        "            gold_labels.append(deprel_dict[sentence['deprel'][i]])\n",
        "    \n",
        "    return sanitized, gold_tree, gold_labels "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a1b7b56-5928-4ca4-9427-0a5edd150ecd",
      "metadata": {
        "id": "5a1b7b56-5928-4ca4-9427-0a5edd150ecd"
      },
      "source": [
        "## Testing parser and oracle..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21bcbd5e-b121-4cef-992f-f22df16e4315",
      "metadata": {
        "id": "21bcbd5e-b121-4cef-992f-f22df16e4315"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "train_dataset = load_dataset('universal_dependencies', 'it_isdt', split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9269d170-abea-4671-b795-a9026fc3e46a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9269d170-abea-4671-b795-a9026fc3e46a",
        "outputId": "ec9297d8-8749-4dc9-d941-d4db89969645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<ROOT>', 'Inconsueto', 'allarme', 'a', 'la', 'Tate', 'Gallery', ':']\n",
            "[-1, 2, 0, 5, 5, 2, 5, 2]\n",
            "[-1, 4, 41, 8, 17, 31, 28, 40]\n"
          ]
        }
      ],
      "source": [
        "deprels = create_deprel_dict(train_dataset)\n",
        "sentence = train_dataset[3]\n",
        "\n",
        "tokens, gold_tree, gold_labels = create_gold(sentence, deprels)\n",
        "\n",
        "print(tokens)\n",
        "print(gold_tree)\n",
        "print(gold_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2e822a0-9ca2-405d-acdb-4b16f4021a7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2e822a0-9ca2-405d-acdb-4b16f4021a7c",
        "outputId": "1d304be9-7952-4691-a24f-b3f94ae2acae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STACK:  ['<ROOT>', 'Inconsueto', 'allarme'] BUFFER:  ['a', 'la', 'Tate', 'Gallery', ':']\n",
            "[(-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None')]\n"
          ]
        }
      ],
      "source": [
        "parser = ArcStandard(tokens, deprels)\n",
        "oracle = Oracle(parser, gold_tree, gold_labels)\n",
        "\n",
        "parser.print_configuration()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3670b9c6-fa88-4258-8b38-7069bb4cb2b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3670b9c6-fa88-4258-8b38-7069bb4cb2b5",
        "outputId": "3ea03c43-6ae9-40d9-ce1d-89f393a1ab3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STACK:  ['<ROOT>', 'allarme'] BUFFER:  ['a', 'la', 'Tate', 'Gallery', ':']\n",
            "[(-1, 'None'), (2, 'amod'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None')]\n",
            "STACK:  ['<ROOT>', 'allarme', 'a'] BUFFER:  ['la', 'Tate', 'Gallery', ':']\n",
            "[(-1, 'None'), (2, 'amod'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None')]\n",
            "STACK:  ['<ROOT>', 'allarme', 'a', 'la'] BUFFER:  ['Tate', 'Gallery', ':']\n",
            "[(-1, 'None'), (2, 'amod'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None')]\n",
            "STACK:  ['<ROOT>', 'allarme', 'a', 'la', 'Tate'] BUFFER:  ['Gallery', ':']\n",
            "[(-1, 'None'), (2, 'amod'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None'), (-1, 'None')]\n",
            "STACK:  ['<ROOT>', 'allarme', 'a', 'Tate'] BUFFER:  ['Gallery', ':']\n",
            "[(-1, 'None'), (2, 'amod'), (-1, 'None'), (-1, 'None'), (5, 'det'), (-1, 'None'), (-1, 'None'), (-1, 'None')]\n",
            "STACK:  ['<ROOT>', 'allarme', 'Tate'] BUFFER:  ['Gallery', ':']\n",
            "[(-1, 'None'), (2, 'amod'), (-1, 'None'), (5, 'case'), (5, 'det'), (-1, 'None'), (-1, 'None'), (-1, 'None')]\n",
            "STACK:  ['<ROOT>', 'allarme', 'Tate', 'Gallery'] BUFFER:  [':']\n",
            "[(-1, 'None'), (2, 'amod'), (-1, 'None'), (5, 'case'), (5, 'det'), (-1, 'None'), (-1, 'None'), (-1, 'None')]\n",
            "STACK:  ['<ROOT>', 'allarme', 'Tate'] BUFFER:  [':']\n",
            "[(-1, 'None'), (2, 'amod'), (-1, 'None'), (5, 'case'), (5, 'det'), (-1, 'None'), (5, 'flat:name'), (-1, 'None')]\n",
            "STACK:  ['<ROOT>', 'allarme'] BUFFER:  [':']\n",
            "[(-1, 'None'), (2, 'amod'), (-1, 'None'), (5, 'case'), (5, 'det'), (2, 'nmod'), (5, 'flat:name'), (-1, 'None')]\n",
            "STACK:  ['<ROOT>', 'allarme', ':'] BUFFER:  []\n",
            "[(-1, 'None'), (2, 'amod'), (-1, 'None'), (5, 'case'), (5, 'det'), (2, 'nmod'), (5, 'flat:name'), (-1, 'None')]\n",
            "STACK:  ['<ROOT>', 'allarme'] BUFFER:  []\n",
            "[(-1, 'None'), (2, 'amod'), (-1, 'None'), (5, 'case'), (5, 'det'), (2, 'nmod'), (5, 'flat:name'), (2, 'punct')]\n",
            "STACK:  ['<ROOT>'] BUFFER:  []\n",
            "[(-1, 'None'), (2, 'amod'), (0, 'root'), (5, 'case'), (5, 'det'), (2, 'nmod'), (5, 'flat:name'), (2, 'punct')]\n"
          ]
        }
      ],
      "source": [
        "while not parser.is_tree_final():  # transition precedence implemented here\n",
        "    if oracle.is_shift_gold():  \n",
        "        parser.shift()\n",
        "    elif oracle.is_left_arc_gold():\n",
        "        parser.left_arc(oracle.get_gold_label())\n",
        "    elif oracle.is_right_arc_gold():\n",
        "        parser.right_arc(oracle.get_gold_label())\n",
        "    \n",
        "    parser.print_configuration()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05840970-630c-4152-bdc6-2c25e81fefff",
      "metadata": {
        "id": "05840970-630c-4152-bdc6-2c25e81fefff"
      },
      "source": [
        "## Testing the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb9e3f8e-bada-4197-b52f-c14a2755df68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb9e3f8e-bada-4197-b52f-c14a2755df68",
        "outputId": "2b5ab80c-6dbc-4e9a-9ce3-ccfeb9c9943d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# remember to call orig.is_projective() to check for projectivity!!\n",
        "sentence = train_dataset[10]\n",
        "deprels = create_deprel_dict(train_dataset)\n",
        "\n",
        "tokens, gold_tree, gold_labels = create_gold(sentence, deprels)\n",
        "orig.is_projective(gold_tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6464ec95-43b7-457a-b764-8ce310c34207",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6464ec95-43b7-457a-b764-8ce310c34207",
        "outputId": "26be50e2-6122-4299-c7e0-e85b541a9a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "non projective:  167\n",
            "correct:  282533\n",
            "wrong:  0\n"
          ]
        }
      ],
      "source": [
        "### NOTE: THIS CELL WAS COPIED WITH FEW MODIFICATIONS FROM THE ORIGINAL FILE\n",
        "\n",
        "# oracle test: run the parser guided by the oracle on the entire training set \n",
        "\n",
        "non_projective = 0\n",
        "correct = 0\n",
        "wrong = 0\n",
        "\n",
        "deprels = create_deprel_dict(train_dataset)\n",
        "\n",
        "for sample in train_dataset:\n",
        "    tokens, gold_tree, gold_labels = create_gold(sample, deprels)\n",
        "\n",
        "    if not orig.is_projective(gold_tree):\n",
        "        non_projective += 1\n",
        "        continue\n",
        "\n",
        "    parser = ArcStandard(tokens, deprels)\n",
        "    oracle = Oracle(parser, gold_tree, gold_labels)\n",
        "\n",
        "    while not parser.is_tree_final():\n",
        "        if oracle.is_left_arc_gold(): \n",
        "            parser.left_arc(oracle.get_gold_label())\n",
        "        elif oracle.is_right_arc_gold():\n",
        "            parser.right_arc(oracle.get_gold_label())\n",
        "        elif oracle.is_shift_gold(): \n",
        "            parser.shift()\n",
        "\n",
        "    for j in range(len(gold_tree)):  # comparing heads from parser and gold for actual sample\n",
        "        if gold_tree[j] == parser.arcs[j][0] and gold_labels[j] == parser.arcs[j][1]: \n",
        "            correct += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "\n",
        "print(\"non projective: \", non_projective)\n",
        "print(\"correct: \", correct)\n",
        "print(\"wrong: \", wrong)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44355fbe-cb0c-4ae5-8967-016ec09033fc",
      "metadata": {
        "id": "44355fbe-cb0c-4ae5-8967-016ec09033fc",
        "outputId": "f87fd0da-15df-4d67-829e-21be6b0ee436"
      },
      "source": [
        "## Creating samples for the neural oracle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "453c9ca5-9c30-4813-bf82-2555efaeac12",
      "metadata": {
        "id": "453c9ca5-9c30-4813-bf82-2555efaeac12"
      },
      "outputs": [],
      "source": [
        "# Modified from the original\n",
        "\"\"\"\n",
        "This function processes a single sample, which is one sentence provided as an element of a Dataset object and returns \n",
        "    enc_sentence : a list of integers encoding the phrase #???? is this even correct for BERT???\n",
        "    gold_path : a list of configurations\n",
        "    gold_moves : a list of (move, label)\n",
        "\"\"\"\n",
        "def process_sample(sample, emb_dictionary, deprels, get_gold_path = False): #emb_dictionary and deprels are dictionaries of words and of dependency relations\n",
        "    tokens, gold_tree, gold_labels = create_gold(sample, deprels)\n",
        "    #print(\"DEBUG:\", tokens)\n",
        "    enc_sentence = [emb_dictionary[word] if word in emb_dictionary else emb_dictionary[\"<unk>\"] for word in tokens]\n",
        "    #print(\"DEBUG:\", enc_sentence)\n",
        "    \n",
        "    # gold_path and gold_moves are parallel arrays whose elements refer to parsing steps\n",
        "    gold_path = []   # record two topmost stack token and first buffer token for current step\n",
        "    gold_moves = []  # oracle (canonical) move for current step: 100 is left, 0 right, -100 shift #motivations provided above TODO\n",
        "\n",
        "    if get_gold_path:  # only for training\n",
        "        parser = ArcStandard(enc_sentence, deprels)\n",
        "        oracle = Oracle(parser, gold_tree, gold_labels)\n",
        "\n",
        "        while not parser.is_tree_final():\n",
        "            configuration = [parser.stack[len(parser.stack)-2], parser.stack[len(parser.stack)-1]]\n",
        "            if len(parser.buffer) == 0:\n",
        "                configuration.append(-1)\n",
        "            else:\n",
        "                configuration.append(parser.buffer[0])\n",
        "\n",
        "            gold_path.append(configuration)\n",
        "            \n",
        "            if oracle.is_left_arc_gold():\n",
        "                label_code = oracle.get_gold_label() #deprels[oracle.get_gold_label()]\n",
        "                parser.left_arc(label_code) #labels 1-45 mean leftarc\n",
        "                gold_moves.append(1+label_code) #note: I switched the instructions here for symmetry. There was no comment or good reason not to.\n",
        "            elif oracle.is_right_arc_gold():\n",
        "                label_code = oracle.get_gold_label() #deprels[oracle.get_gold_label()]\n",
        "                parser.right_arc(1 + len(deprels) + label_code) #labels 46-90 mean rightarc\n",
        "                gold_moves.append(1 + len(deprels) + label_code)\n",
        "            elif oracle.is_shift_gold():\n",
        "                parser.shift()\n",
        "                gold_moves.append(0)\n",
        "            else:\n",
        "                print(\"**** AN ERROR OCCURRED ****\")\n",
        "                #print(tokens)\n",
        "                #print(enc_sentence)\n",
        "                #print(gold_tree)\n",
        "                parser.print_configuration()\n",
        "                raise Exception(\"No action identified as gold!! Please make sure your dataset doesn't contain nonprojective trees.\")\n",
        "                \n",
        "            #print(\"DEBUG: \", parser.stack)\n",
        "    \n",
        "    #print(\"DEBUG: length of path: \", len(gold_path), \"length of moves: \", len(gold_moves))\n",
        "    return enc_sentence, gold_path, gold_moves, gold_tree, gold_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d214f7df-0622-40df-9ce6-ace620105cc2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d214f7df-0622-40df-9ce6-ace620105cc2",
        "outputId": "6929f3d7-e5c6-42d3-fd2a-ee2541487ecc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest_sentence = train_dataset[88]\\nemb_dictionary = create_dict(train_dataset)\\ndeprels = create_deprel_dict(train_dataset)\\n\\nprocess_sample(test_sentence, emb_dictionary, deprels, get_gold_path = True)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "### Code to test process_sample\n",
        "\"\"\"\n",
        "test_sentence = train_dataset[88]\n",
        "emb_dictionary = create_dict(train_dataset)\n",
        "deprels = create_deprel_dict(train_dataset)\n",
        "\n",
        "process_sample(test_sentence, emb_dictionary, deprels, get_gold_path = True)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf6e832-86cf-4033-8524-1a1070e7e5a1",
      "metadata": {
        "id": "abf6e832-86cf-4033-8524-1a1070e7e5a1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Function that prepares the data for the model.\n",
        "@return sentences : list of sentences in the dataset encoded according to emb_dictionary\n",
        "@return paths : list of lists of configurations visited during the oracle-guided parsing (on training data) | empty list if get_gold_path is False\n",
        "@return moves : list of lists of moves as a number performed during the oracle-guided parsing (on training data) | empty if get_gold_path is False\n",
        "                                N.B. 0 = shift\n",
        "                                        1-45 = leftarc + label\n",
        "                                        46-90 = rightarc + label\n",
        "@return trees : ground truth tree\n",
        "@return labels : ground truth lables\n",
        "\"\"\"\n",
        "#modified form orig\n",
        "def prepare_batch(batch_data, emb_dictionary, deprels, get_gold_path=False):\n",
        "    data = [process_sample(s, emb_dictionary, deprels, get_gold_path=get_gold_path) for s in batch_data]\n",
        "    # sentences, paths, moves, trees are parallel arrays, each element refers to a sentence\n",
        "    sentences = [s[0] for s in data]\n",
        "    paths = [s[1] for s in data]\n",
        "    moves = [s[2] for s in data]\n",
        "    trees = [s[3] for s in data]\n",
        "    labels = [s[4] for s in data]\n",
        "    return sentences, paths, moves, trees, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f3002b0-00f7-4f95-82f7-215b97ada014",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f3002b0-00f7-4f95-82f7-215b97ada014",
        "outputId": "ccc7d9ca-8810-4f71-818f-9234216586aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nemb_dictionary = create_dict(train_dataset)\\ndeprels = create_deprel_dict(train_dataset)\\n\\n_ = prepare_batch(iter(train_dataset), emb_dictionary, deprels, True)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "### code to test prepare_batch\n",
        "\"\"\"\n",
        "emb_dictionary = create_dict(train_dataset)\n",
        "deprels = create_deprel_dict(train_dataset)\n",
        "\n",
        "_ = prepare_batch(iter(train_dataset), emb_dictionary, deprels, True)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e3cfb5-dad2-48a3-bc92-cf2196d8748f",
      "metadata": {
        "id": "c7e3cfb5-dad2-48a3-bc92-cf2196d8748f"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Copied from original.\n",
        "Modified casing of pad and unk.\n",
        "\"\"\"\n",
        "def create_dict(dataset, threshold=3):\n",
        "  dic = {}  # dictionary of word counts\n",
        "  for sample in dataset:\n",
        "    for word in sample['tokens']:\n",
        "      if word in dic:\n",
        "        dic[word] += 1\n",
        "      else:\n",
        "        dic[word] = 1 \n",
        "\n",
        "  map = {}  # dictionary of word/index pairs\n",
        "  map[\"[PAD]\"] = 0\n",
        "  map[\"<ROOT>\"] = 1\n",
        "  map[\"<unk>\"] = 2\n",
        "\n",
        "  next_indx = 3\n",
        "  for word in dic.keys():\n",
        "    if dic[word] >= threshold:\n",
        "      map[word] = next_indx\n",
        "      next_indx += 1\n",
        "\n",
        "  return map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dc8dfb6-91e4-425d-8406-bd427b31d053",
      "metadata": {
        "id": "0dc8dfb6-91e4-425d-8406-bd427b31d053"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Return the indices of the element to mantain in the dataset. This procedure was\n",
        "created because some sentences present some dependecies never seen in the training\n",
        "and will not processed correctly.\n",
        "In our case the worng sentences have the following idx element: 10_new-83\n",
        "\"\"\"\n",
        "def remove_sentences(dataset, idx):\n",
        "    ret = []\n",
        "    for i in range(len(dataset)):\n",
        "        x = dataset[i]\n",
        "        if x[\"idx\"] != idx:\n",
        "            ret.append(i)\n",
        "    return ret\n",
        "\n",
        "\"\"\"\n",
        "Return indices of projective sentences in the dataset. Must be used with select() to filter out nonprojective trees.\n",
        "\"\"\"\n",
        "def find_projective_idx(dataset):\n",
        "    ret = []\n",
        "    for i in range(len(dataset)):\n",
        "        x = dataset[i]\n",
        "        if orig.is_projective([-1] + [int(head) for head in x[\"head\"] if head!='None']):\n",
        "            ret.append(i)\n",
        "    return ret\n",
        "\n",
        "\"\"\"\n",
        "Return indices of sentences shorter than TOKENIZER_MAX_LEN\n",
        "\"\"\"\n",
        "def find_long_idx(dataset):\n",
        "    ret = []\n",
        "    for i in range(len(dataset)):\n",
        "        x = dataset[i]\n",
        "        if len(x[\"tokens\"]) <= TOKENIZER_MAX_LEN: \n",
        "            ret.append(i)\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1491189-7f64-445d-a7ab-b556101fbc96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1491189-7f64-445d-a7ab-b556101fbc96",
        "outputId": "4954e7ca-11cc-442c-d8f9-f29fbfc0611f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset universal_dependencies (/root/.cache/huggingface/datasets/universal_dependencies/it_isdt/2.7.0/065e728dfe9a8371434a6e87132c2386a6eacab1a076d3a12aa417b994e6ef7d)\n",
            "Reusing dataset universal_dependencies (/root/.cache/huggingface/datasets/universal_dependencies/it_isdt/2.7.0/065e728dfe9a8371434a6e87132c2386a6eacab1a076d3a12aa417b994e6ef7d)\n",
            "Reusing dataset universal_dependencies (/root/.cache/huggingface/datasets/universal_dependencies/it_isdt/2.7.0/065e728dfe9a8371434a6e87132c2386a6eacab1a076d3a12aa417b994e6ef7d)\n"
          ]
        }
      ],
      "source": [
        "train_dataset = load_dataset('universal_dependencies', 'it_isdt', split=\"train\")\n",
        "dev_dataset = load_dataset('universal_dependencies', 'it_isdt', split=\"validation\")\n",
        "test_dataset = load_dataset('universal_dependencies', 'it_isdt', split=\"test\")\n",
        "\n",
        "train_dataset = train_dataset.select(find_projective_idx(train_dataset)) #to remove nonprojective trees\n",
        "train_dataset = train_dataset.select(find_long_idx(train_dataset)) #to remove sentences too long\n",
        "\n",
        "dev_dataset = dev_dataset.select(find_projective_idx(dev_dataset)) #to remove nonprojective trees\n",
        "dev_dataset = dev_dataset.select(find_long_idx(dev_dataset)) #to remove sentences too long\n",
        "\n",
        "test_dataset = test_dataset.select(remove_sentences(test_dataset, \"10_new-83\")) #to remove sentences that have labels that do not appear in the training set\n",
        "test_dataset = test_dataset.select(find_projective_idx(test_dataset)) #to remove nonprojective trees\n",
        "test_dataset = test_dataset.select(find_long_idx(test_dataset)) #to remove sentences too long"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bcb2cbf-eebb-4099-bba8-a05925919abf",
      "metadata": {
        "id": "8bcb2cbf-eebb-4099-bba8-a05925919abf"
      },
      "outputs": [],
      "source": [
        "emb_dictionary = create_dict(train_dataset)\n",
        "deprels = create_deprel_dict(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d176de-aa6a-4258-ab75-e46c772beb81",
      "metadata": {
        "id": "51d176de-aa6a-4258-ab75-e46c772beb81"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=partial(prepare_batch, emb_dictionary = emb_dictionary, deprels = deprels, get_gold_path=True))\n",
        "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=partial(prepare_batch, emb_dictionary = emb_dictionary, deprels = deprels, get_gold_path=True))\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=partial(prepare_batch, emb_dictionary = emb_dictionary, deprels = deprels, get_gold_path=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d5fbde0-8a1c-4930-9a63-97b5ad18108b",
      "metadata": {
        "id": "1d5fbde0-8a1c-4930-9a63-97b5ad18108b"
      },
      "source": [
        "## Ideas for the future\n",
        "* Use mean square error to predict pairs (MOVE, LABEL) in the training set\n",
        "* We need to rescale the data in an acceptable range. Probably should make the MOVE have greater variation, e.g LeftArc=100 RightArc=0 Shift=-100\n",
        "* BERT layer must process each sentence separately to produce word embeddings, but it should also be finetuned alongside the fully connected layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xNoPh6s17yyx",
      "metadata": {
        "id": "xNoPh6s17yyx"
      },
      "source": [
        "# Neural Oracle\n",
        "\n",
        "Neural oracle implemented using BERT fine-tunned with a simple classifiers.\n",
        "The references are: [Towards Data Science](https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f) and [Text Classification | Sentiment Analysis with BERT using huggingface, PyTorch and Python Tutorial](https://www.youtube.com/watch?v=8N-nM3QW7O0).\n",
        "\n",
        "The hyperparameters and the training tweaks are taken from the original BERT paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZfqzDHLLA08V",
      "metadata": {
        "id": "ZfqzDHLLA08V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "eecde51cb7e54d0f848ab9ef3726d839",
            "8488d52b10574aaeab67f83e262358d4",
            "cf43319269b14da2976c4fc7795f5bde",
            "accce3967b0948feaf59d14395b709da",
            "0e2f4df1d92d411f980171bd3ce64e52",
            "ffc35f29b9ae4fcbb3ac6ec0005919f9",
            "3896522433324259b164d9f8d46580b8",
            "1770d31130314217a5be4e8164c6d38f",
            "ef4ab0640db745bdbdbaf50e0e4e9cf2",
            "bbf3f9f814a74c029efba787e5e0f83e",
            "35d2e80226364e489e465a8b7b27b347",
            "f3119062a560455b8d00259dce228b46",
            "75f90470c1104c5caadeb506617f891d",
            "632378d01ad14d298409757629fa4df0",
            "f480217529e64edd839cb4fa89e6feec",
            "94e87e1fb5c4431db629c272b03a33b5",
            "6457666b57fd47f694e8ecf52584dc53",
            "593db636aa3f4546ae12c852e66dfb6a",
            "f3895ec4b0f64eee8c3faa121b2447a3",
            "def6589e087049669155dac137412377",
            "ec0dbed953b74a77b4c7e8b1d7bc1cbd",
            "1e26c4867909440ea807d42d49391491",
            "d4228a20ff654cbfbab69e64cfe344e4",
            "3143410270cc4906bbbeef15364d2f49",
            "f74aa7b8cb27472d87377d4c1c60f00f",
            "1eae9e5840d24de4923bfd0b364471f7",
            "c01d0d31320a4116ad6853806dd20166",
            "e9e98c1f127f498298c8819e61a9c205",
            "646cbcb003fb4fc9bf440f871d4dd1ae",
            "5bc8c894a8734773b9715ef885e1a00b",
            "8808af483ce94d6da0058f819da37541",
            "35e73507118e4f69aa575b53a94a589d",
            "9228e36833004217ab450b9a390fea8b"
          ]
        },
        "outputId": "1dfae95b-0a09-4486-dd4a-b0b8ac5bd070"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/230k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eecde51cb7e54d0f848ab9ef3726d839"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3119062a560455b8d00259dce228b46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4228a20ff654cbfbab69e64cfe344e4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# load the BERT tokenizer with pretrained weights for the Italian language\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "845TJICw2Q9x",
      "metadata": {
        "id": "845TJICw2Q9x"
      },
      "source": [
        "In this section we have loaded the BERT tokenizer and try it to see how a sentence is processed and which kind of objects the tokenizer returns to be used in BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QtZZfEGK9IG5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtZZfEGK9IG5",
        "outputId": "f364f4b5-fb7a-4240-d185-3e171b418d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  102,   401,   162,  1909,   532, 27948,  2369,   139,  5101,  1783,\n",
            "           223,  1731,   146, 16711,   103]])\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "[CLS] Con il termine interfluvio in geologia si indica la porzione [SEP]\n"
          ]
        }
      ],
      "source": [
        "example_text = '''Con il termine interfluvio in geologia si indica la porzione di superﬁcie più elevata\n",
        "                  che separa due valli ﬂuviali adiacenti, che può essere una cresta oppure un' area ampia,\n",
        "                  comunque non coinvolta dal movimento delle acque'''\n",
        "\n",
        "\n",
        "bert_input = tokenizer(example_text, padding='max_length', max_length = 15, \n",
        "                       truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "print(bert_input['input_ids'])      # words to integer\n",
        "print(bert_input['token_type_ids']) # binary mask that identifies whether a token is in the first sentence (before [SEP]) or the second (after [SEP])\n",
        "print(bert_input['attention_mask']) # binary mask that identifies whether a token is a real word or just padding\n",
        "\n",
        "decoded_text = tokenizer.decode(bert_input.input_ids[0])\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mxtuOhzE6rTi",
      "metadata": {
        "id": "mxtuOhzE6rTi"
      },
      "source": [
        "  ## Oracle model using BERT\n",
        "For the oracle we decided to use and fine-tune bert in a regression task.\n",
        "The main idea is:\n",
        "\n",
        "*   given a sentence get the **'Bert input tokens**' and the **'attention mask**' from the BERT tokenizer above;\n",
        "*   pass the 'input tokens' and the 'attention mask' to the BERTOracle that returns a tensor that contains a list of configurations, one for each sentence.\n",
        "\n",
        "In this setting a configuration is a pair of numbers. For this reason we use MSE loss to create a **multi-ouptut regressor** task.\n",
        "\n",
        "One configuration is in the form (**ACTION, LABELS**) where for ACTION we have:\n",
        "\n",
        "* 100 for **LeftArc** action;\n",
        "* 0 for **RightArc** action;\n",
        "* -100 for **Shift** action.\n",
        "\n",
        "For the LABELS we have decided to order them in alphabetical order and convert them into consecutive numbers.\n",
        "The main idea is: \n",
        "\n",
        "the labels represent the relation between words.<br>\n",
        "In the used treebank there are many such relation that are very similar like *obj* and *iobj* so by ordering them and giving consecutive numbers we have that *obj* will be encoded, for instance, as 12 and *iobj* with 13. This means that, even if the oracle infere 13 instead of 12 we have that the prediction is not as wrong as if it will have predicte 50.<br>\n",
        "The main assumption is that closer numbers represent more similar relation.\n",
        "\n",
        "To perfrom the actual multi-output regression we used a simple linear layer that outputs this two values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KkXDGmBW-vFo",
      "metadata": {
        "id": "KkXDGmBW-vFo"
      },
      "outputs": [],
      "source": [
        "class BERTOracle(nn.Module):\n",
        "    def __init__(self, input_size, deprels):\n",
        "      super(BERTOracle, self).__init__()\n",
        "      self.bert = BertModel.from_pretrained(BERT_MODEL)\n",
        "      self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "      self.w1 = nn.Linear(input_size, MLP_SIZE) #for classification: we need 2*sizeof(label_set)+1\n",
        "      self.activation = torch.nn.Tanh()\n",
        "      self.w2 = nn.Linear(MLP_SIZE, 2*len(deprels)+1)\n",
        "      self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    #Substitute the \"tokenized words\" in the paths with the embeddings\n",
        "    def substitute_embeddings(self, word_embeddings, paths):\n",
        "      ffn_input = []\n",
        "      zero_tensor = torch.zeros(768, requires_grad=False).to(DEVICE)\n",
        "      for sentence_index in range(len(paths)): # one path for each sentence so len(paths) = number of sentences\n",
        "        for configuration in paths[sentence_index]: # take the sentence in number 'sentence_index'\n",
        "          new_tensor = torch.cat(\n",
        "              [\n",
        "                zero_tensor if configuration[0]==-1 else word_embeddings[sentence_index][configuration[0]], \n",
        "                zero_tensor if configuration[1]==-1 else word_embeddings[sentence_index][configuration[1]], \n",
        "                zero_tensor if configuration[2]==-1 else word_embeddings[sentence_index][configuration[2]]\n",
        "              ]\n",
        "            )\n",
        "          ffn_input.append(new_tensor)\n",
        "      ffn_input = torch.stack(ffn_input).to(DEVICE)\n",
        "      return ffn_input\n",
        "\n",
        "    #Since each sentence have a different number of configuration I pad the paths to have all the same number of configurations\n",
        "    #Maybe its not required\n",
        "    def pad_paths(arr):\n",
        "      pad_token = [-1,-1,-1]\n",
        "      padded = zip(*itertools.zip_longest(*arr, fillvalue=pad_token))\n",
        "      return list(padded)\n",
        "\n",
        "    def ffn_pass(self, configuration):\n",
        "      out = self.dropout(configuration)\n",
        "      out = self.w1(out)\n",
        "      out = self.activation(out)\n",
        "      out = self.dropout(out)\n",
        "      out = self.w2(out)\n",
        "      return self.softmax(out)\n",
        "\n",
        "    def forward(self, input_tokens, attention_mask, paths):\n",
        "      #Compute the BERT embeddings and retrieve them from the last hidden layer\n",
        "      out = self.bert(input_tokens, attention_mask)    \n",
        "      word_embeddings = out.last_hidden_state # [sentence_index, tokens, embedding_of_token]\n",
        "\n",
        "      #Substitute each word in a configuration with the corrisponding embedding.\n",
        "      #Be aware of: we need padding to have the configurations all the same size, maybe we need to reshape them to match (batch_size x config_per_sentence x 768*3)\n",
        "      configurations = self.substitute_embeddings(word_embeddings, paths)\n",
        "\n",
        "      #Pass through FFN\n",
        "      return self.ffn_pass(configurations)\n",
        "    ##################################################################################################################\n",
        "    \n",
        "    def infere(self, sentences):\n",
        "        parsers = [ArcStandard(i, deprels) for i in sentences]\n",
        "        #copied from below...\n",
        "        list_sentences = [str(x) for x in sentences] \n",
        "        preprocessed_text = tokenizer(list_sentences, padding='max_length', max_length = TOKENIZER_MAX_LEN, truncation=True, return_tensors=\"pt\")\n",
        "        input_tokens = preprocessed_text[\"input_ids\"].to(DEVICE)\n",
        "        attention_mask = preprocessed_text[\"attention_mask\"].to(DEVICE)\n",
        "        \n",
        "        h = self.bert(input_tokens, attention_mask).last_hidden_state\n",
        "\n",
        "        while not self.parsed_all(parsers):\n",
        "            configurations = self.get_configurations(parsers)\n",
        "            mlp_input = self.substitute_embeddings(h, configurations) \n",
        "            mlp_out = self.ffn_pass(mlp_input)\n",
        "            self.parse_step(parsers, mlp_out, deprels)\n",
        "\n",
        "        return [parser.arcs for parser in parsers]\n",
        "    \n",
        "    \"\"\"\n",
        "    This function was copied from the original file\n",
        "    \"\"\"\n",
        "    def get_configurations(self, parsers):\n",
        "        configurations = []\n",
        "\n",
        "        for parser in parsers:\n",
        "          if parser.is_tree_final():\n",
        "            conf = [-1, -1, -1]\n",
        "          else:\n",
        "            conf = [parser.stack[len(parser.stack)-2], parser.stack[len(parser.stack)-1]]\n",
        "            if len(parser.buffer) == 0:\n",
        "              conf.append(-1)\n",
        "            else:\n",
        "              conf.append(parser.buffer[0])  \n",
        "          configurations.append([conf])\n",
        "\n",
        "        return configurations\n",
        "\n",
        "    \"\"\"\n",
        "    This function was copied from the original file\n",
        "    \"\"\"\n",
        "    def parsed_all(self, parsers):\n",
        "        for parser in parsers:\n",
        "            if not parser.is_tree_final():\n",
        "                return False\n",
        "        return True\n",
        "    \n",
        "    \"\"\"\n",
        "    Remember we use the convention\n",
        "    0 = shift\n",
        "    [1, len(deprels)] = left-arc\n",
        "    [len(deprels)+1, 2*len(deprels)+1] = right-arc\n",
        "    \n",
        "    This function was adapted from the original to remove padding from the buffer avoiding all edge cases\n",
        "    that complicated the original code.\n",
        "    \"\"\"\n",
        "    def parse_step(self, parsers, moves, labels):\n",
        "      moves_argm = moves.argmax(-1) #vector of argmaxes\n",
        "      moves_second_argm = moves[:, 1:].argmax(-1) + 1 #needed when buffer is empty but argmax is 0 (shift). Contains the argmax for each sentence excluding index 0. \n",
        "                                                      #1 is added to account for slicing\n",
        "      \n",
        "      for i in range(len(parsers)):\n",
        "          kind_of_move = 0 if moves_argm[i] == 0 else (1 + int(moves_argm[i] / len(labels))) #0 if shift, 1 if leftarc, 2 is rightarc\n",
        "          label = moves_argm[i] % len(labels) #label of the arc, ignored if shift\n",
        "          \n",
        "          while parsers[i].buffer and parsers[i].buffer[-1] == 0: #while buffer not empty and we have padding in the buffer: remove it.\n",
        "              parsers[i].buffer.pop()\n",
        "              continue #a new prediction is needed\n",
        "\n",
        "          if parsers[i].is_tree_final():\n",
        "              continue\n",
        "\n",
        "          if kind_of_move == 1: #predicted: leftarc\n",
        "              parsers[i].left_arc(label)\n",
        "\n",
        "          elif kind_of_move == 2: #predicted: rightarc\n",
        "              parsers[i].right_arc(label)\n",
        "\n",
        "          elif moves_argm[i] == 0: #predicted: shift\n",
        "              if parsers[i].buffer: \n",
        "                  parsers[i].shift()\n",
        "              else: #in case buffer is empty\n",
        "                  if moves_second_argm[i]/len(labels) == 0:\n",
        "                      parsers[i].left_arc(moves_second_argm % len(labels))\n",
        "                  elif moves_second_argm[i]/len(labels) == 1:\n",
        "                      parsers[i].right_arc(moves_second_argm % len(labels))\n",
        "                  else:\n",
        "                      raise Exception(\"moves_second_argm not in range [1, 2*len(labels)]\")\n",
        "                      \n",
        "          else:\n",
        "              raise Exception(\"argmax not in range [0, 2*len(labels)]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oUpbfnjJBTvv",
      "metadata": {
        "id": "oUpbfnjJBTvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fde1c0f-4a0a-4946-d384-0ce192cb4c0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTOracle(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(32102, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (w1): Linear(in_features=2304, out_features=300, bias=True)\n",
              "  (activation): Tanh()\n",
              "  (w2): Linear(in_features=300, out_features=91, bias=True)\n",
              "  (softmax): Softmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "#Initialize the BERTOracle model\n",
        "model = BERTOracle(INPUT_SIZE, deprels)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QRk1Qh53F5EG",
      "metadata": {
        "id": "QRk1Qh53F5EG"
      },
      "source": [
        "## Training and evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AaJXSRebF7EE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaJXSRebF7EE",
        "outputId": "b02d0dbc-ebe6-475a-9500-05ac645a466e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# AdamW optimizer that uses a different weight decay procedure than Adam\n",
        "optimizer = AdamW(model.parameters(), lr = LEARNING_RATE, correct_bias = False)\n",
        "\n",
        "total_steps = len(train_dataloader) * EPOCHS\n",
        "\n",
        "# Linear scheduler: used to decay the learning rates to improve training performacnes.\n",
        "scheduler  = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = 0,\n",
        "    num_training_steps = total_steps\n",
        ")\n",
        "#Define the loss as cross entropy\n",
        "loss_fn = nn.CrossEntropyLoss().to(DEVICE) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p3ePzCq4gJWx",
      "metadata": {
        "id": "p3ePzCq4gJWx"
      },
      "source": [
        "## Train and evaluation function for BERTOracle model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4h-Y7lj8INBz",
      "metadata": {
        "id": "4h-Y7lj8INBz"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, scheduler):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  for batch in data_loader:\n",
        "    sentences, paths, moves, trees, labels = batch\n",
        "    optimizer.zero_grad()                                             # initialize gradient to zeros\n",
        "    list_sentences = [str(x) for x in sentences]                      # list of sentences in string format\n",
        "\n",
        "    preprocessed_text = tokenizer(list_sentences, padding='max_length', max_length = TOKENIZER_MAX_LEN, \n",
        "                       truncation=True, return_tensors=\"pt\")          # tokenize input using BERT tokenizer\n",
        "\n",
        "    input_tokens = preprocessed_text[\"input_ids\"].to(DEVICE)\n",
        "    attention_mask = preprocessed_text[\"attention_mask\"].to(DEVICE)\n",
        "  \n",
        "    outputs = model(input_tokens, attention_mask, paths)\n",
        "\n",
        "    # Compute the loss for each configurations for each sentence in parallel\n",
        "    tensor_moves = torch.tensor(sum(moves, [])).to(DEVICE)\n",
        "    loss = loss_fn(outputs, tensor_moves)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    #backpropagation routine\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0) # gradient clipping to avoid exploding gradients if they become too large\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "  mean_loss = np.mean(losses)\n",
        "\n",
        "  return mean_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "idw5no5rK0Ea",
      "metadata": {
        "id": "idw5no5rK0Ea"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, optimzier, scheduler):\n",
        "  model = model.eval()                                                # dropout and batch_norm are not enabled\n",
        "\n",
        "  losses = []\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      sentences, paths, moves, trees, labels = batch\n",
        "      optimizer.zero_grad()                                             # initialize gradient to zeros\n",
        "      list_sentences = [str(x) for x in sentences]                      # list of sentences in string format\n",
        "\n",
        "      preprocessed_text = tokenizer(list_sentences, padding='max_length', max_length = TOKENIZER_MAX_LEN, \n",
        "                        truncation=True, return_tensors=\"pt\")           # tokenize input using BERT tokenizer\n",
        "\n",
        "      input_tokens = preprocessed_text[\"input_ids\"].to(DEVICE)\n",
        "      attention_mask = preprocessed_text[\"attention_mask\"].to(DEVICE)\n",
        "\n",
        "      outputs = model(input_tokens, attention_mask, paths)\n",
        "\n",
        "      # Compute the loss for each configurations for each sentence in parallel\n",
        "      tensor_moves = torch.tensor(sum(moves, [])).to(DEVICE)\n",
        "      loss = loss_fn(outputs, tensor_moves)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  mean_loss = np.mean(losses)\n",
        "\n",
        "  return mean_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4qhZPXfxLo67",
      "metadata": {
        "id": "4qhZPXfxLo67"
      },
      "source": [
        "## Training loop\n",
        "Training loop that for the given number of epoch preform the training step and the evaluation step for each sentence in the train_dataloader and val_dataloader.\n",
        "All the results are krept inside a dictionary of history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ngRlytTZLqnK",
      "metadata": {
        "id": "ngRlytTZLqnK"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "history = defaultdict(list) # store training and validation losses in a dictionary \n",
        "epoch_to_save = 5           # save model every 5 epochs\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'Epoch {epoch + 1} / {EPOCHS}')\n",
        "\n",
        "  #Training step\n",
        "  train_loss = train_epoch(model, train_dataloader, loss_fn, optimizer, scheduler)\n",
        "\n",
        "  print(f\"Train loss {train_loss}\")\n",
        "\n",
        "  #Evaluation step\n",
        "  val_loss = eval_model(model, dev_dataloader, loss_fn, optimizer, scheduler)\n",
        "\n",
        "  print(f\"Val loss {val_loss}\")\n",
        "  print('-'*10)\n",
        "\n",
        "  history['train_loss'].append(train_loss)\n",
        "\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if epoch % epoch_to_save == 0:\n",
        "    torch.save(model, SAVE_PATH)\n",
        "    print(\"|MODEL SAVED|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Om8sss6qfK-L",
      "metadata": {
        "id": "Om8sss6qfK-L"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cb5cbce",
      "metadata": {
        "id": "2cb5cbce"
      },
      "outputs": [],
      "source": [
        "#Load the saved model's weights\n",
        "model = torch.load(SAVE_PATH)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0a01376-3579-469e-b907-40e686da5051",
      "metadata": {
        "id": "a0a01376-3579-469e-b907-40e686da5051"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Functions were adapted from the ones in the original notebook\n",
        "preds is a list of *couples*\n",
        "\n",
        "evaluate() returns a couple of values (UAS, LAS)\n",
        "\"\"\"\n",
        "def evaluate(gold, preds): \n",
        "  total = 0\n",
        "  correct_unlabeled = 0\n",
        "  correct_labeled = 0\n",
        "\n",
        "  for g, p in zip(gold, preds):\n",
        "    for i in range(1,len(g)):\n",
        "      total += 1\n",
        "      if g[i][0] == p[i][0]:\n",
        "        correct_unlabeled += 1\n",
        "        if g[i][1] == p[i][1]:\n",
        "          correct_labeled += 1\n",
        "  return correct_unlabeled/total, correct_labeled/total\n",
        "\n",
        "def validation(model, dataloader):\n",
        "  model.eval()\n",
        "  gold = []\n",
        "  preds = []\n",
        "\n",
        "  for batch in dataloader:\n",
        "    sentences, paths, moves, trees, labels = batch\n",
        "    with torch.no_grad():\n",
        "      pred = model.infere(sentences) #now pred is made of (move, label) pairs\n",
        "      \n",
        "      gold += trees\n",
        "      preds += pred\n",
        "          \n",
        "  return evaluate(gold, preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uas, las = validation(model, test_dataloader)"
      ],
      "metadata": {
        "id": "JIWowQo7wcjb"
      },
      "id": "JIWowQo7wcjb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "49ac7b5d-012a-40dc-9d04-6ab4cfcca9a5",
        "05840970-630c-4152-bdc6-2c25e81fefff",
        "44355fbe-cb0c-4ae5-8967-016ec09033fc"
      ],
      "name": "our_dep_parser.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eecde51cb7e54d0f848ab9ef3726d839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8488d52b10574aaeab67f83e262358d4",
              "IPY_MODEL_cf43319269b14da2976c4fc7795f5bde",
              "IPY_MODEL_accce3967b0948feaf59d14395b709da"
            ],
            "layout": "IPY_MODEL_0e2f4df1d92d411f980171bd3ce64e52"
          }
        },
        "8488d52b10574aaeab67f83e262358d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffc35f29b9ae4fcbb3ac6ec0005919f9",
            "placeholder": "​",
            "style": "IPY_MODEL_3896522433324259b164d9f8d46580b8",
            "value": "Downloading: 100%"
          }
        },
        "cf43319269b14da2976c4fc7795f5bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1770d31130314217a5be4e8164c6d38f",
            "max": 235127,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef4ab0640db745bdbdbaf50e0e4e9cf2",
            "value": 235127
          }
        },
        "accce3967b0948feaf59d14395b709da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbf3f9f814a74c029efba787e5e0f83e",
            "placeholder": "​",
            "style": "IPY_MODEL_35d2e80226364e489e465a8b7b27b347",
            "value": " 230k/230k [00:00&lt;00:00, 872kB/s]"
          }
        },
        "0e2f4df1d92d411f980171bd3ce64e52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffc35f29b9ae4fcbb3ac6ec0005919f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3896522433324259b164d9f8d46580b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1770d31130314217a5be4e8164c6d38f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef4ab0640db745bdbdbaf50e0e4e9cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bbf3f9f814a74c029efba787e5e0f83e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d2e80226364e489e465a8b7b27b347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3119062a560455b8d00259dce228b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75f90470c1104c5caadeb506617f891d",
              "IPY_MODEL_632378d01ad14d298409757629fa4df0",
              "IPY_MODEL_f480217529e64edd839cb4fa89e6feec"
            ],
            "layout": "IPY_MODEL_94e87e1fb5c4431db629c272b03a33b5"
          }
        },
        "75f90470c1104c5caadeb506617f891d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6457666b57fd47f694e8ecf52584dc53",
            "placeholder": "​",
            "style": "IPY_MODEL_593db636aa3f4546ae12c852e66dfb6a",
            "value": "Downloading: 100%"
          }
        },
        "632378d01ad14d298409757629fa4df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3895ec4b0f64eee8c3faa121b2447a3",
            "max": 59,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_def6589e087049669155dac137412377",
            "value": 59
          }
        },
        "f480217529e64edd839cb4fa89e6feec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec0dbed953b74a77b4c7e8b1d7bc1cbd",
            "placeholder": "​",
            "style": "IPY_MODEL_1e26c4867909440ea807d42d49391491",
            "value": " 59.0/59.0 [00:00&lt;00:00, 1.56kB/s]"
          }
        },
        "94e87e1fb5c4431db629c272b03a33b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6457666b57fd47f694e8ecf52584dc53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "593db636aa3f4546ae12c852e66dfb6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3895ec4b0f64eee8c3faa121b2447a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "def6589e087049669155dac137412377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec0dbed953b74a77b4c7e8b1d7bc1cbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e26c4867909440ea807d42d49391491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4228a20ff654cbfbab69e64cfe344e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3143410270cc4906bbbeef15364d2f49",
              "IPY_MODEL_f74aa7b8cb27472d87377d4c1c60f00f",
              "IPY_MODEL_1eae9e5840d24de4923bfd0b364471f7"
            ],
            "layout": "IPY_MODEL_c01d0d31320a4116ad6853806dd20166"
          }
        },
        "3143410270cc4906bbbeef15364d2f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9e98c1f127f498298c8819e61a9c205",
            "placeholder": "​",
            "style": "IPY_MODEL_646cbcb003fb4fc9bf440f871d4dd1ae",
            "value": "Downloading: 100%"
          }
        },
        "f74aa7b8cb27472d87377d4c1c60f00f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bc8c894a8734773b9715ef885e1a00b",
            "max": 433,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8808af483ce94d6da0058f819da37541",
            "value": 433
          }
        },
        "1eae9e5840d24de4923bfd0b364471f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35e73507118e4f69aa575b53a94a589d",
            "placeholder": "​",
            "style": "IPY_MODEL_9228e36833004217ab450b9a390fea8b",
            "value": " 433/433 [00:00&lt;00:00, 12.7kB/s]"
          }
        },
        "c01d0d31320a4116ad6853806dd20166": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9e98c1f127f498298c8819e61a9c205": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "646cbcb003fb4fc9bf440f871d4dd1ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bc8c894a8734773b9715ef885e1a00b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8808af483ce94d6da0058f819da37541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35e73507118e4f69aa575b53a94a589d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9228e36833004217ab450b9a390fea8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}